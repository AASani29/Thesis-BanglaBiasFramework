# BanglaMedBias - Part 1: Dataset Preparation

**Project:** Creating the first comprehensive Bangla medical bias evaluation dataset  
**Stage:** Part 1 - Dataset Selection & Filtering (Before Translation)  
**Target:** 500 high-quality clinical vignettes from MedQA-USMLE

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Project Structure](#project-structure)
3. [Installation](#installation)
4. [Usage Guide](#usage-guide)
5. [What You Need to Provide](#what-you-need-to-provide)
6. [Troubleshooting](#troubleshooting)
7. [Next Steps](#next-steps)

---

## ğŸ¯ Project Overview

This is **Part 1** of the BanglaMedBias project, which focuses on:

âœ… Setting up the project structure  
âœ… Downloading source datasets (MedQA-USMLE & AMQA)  
âœ… Exploring and understanding the data  
âœ… Filtering clinical vignettes based on quality criteria  
âœ… Selecting 500 vignettes using stratified sampling  
âœ… Quality validation before translation

**What This Part Does NOT Include:**

- Translation (Part 2)
- Expert validation (Part 3)
- Bias testing (Part 4)

---

## ğŸ“ Project Structure

```
Implementation Part 1/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Downloaded datasets (not in git)
â”‚   â”œâ”€â”€ filtered/               # Filtered vignettes
â”‚   â”œâ”€â”€ translated/             # (Future: translations)
â”‚   â”œâ”€â”€ validated/              # (Future: expert validation)
â”‚   â””â”€â”€ final/                  # (Future: final dataset)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download/
â”‚   â”‚   â””â”€â”€ download_medqa.py   # Download MedQA & AMQA
â”‚   â”œâ”€â”€ filter/
â”‚   â”‚   â”œâ”€â”€ explore_data.py     # Explore dataset structure
â”‚   â”‚   â”œâ”€â”€ filter_vignettes.py # Filter & select 500 vignettes
â”‚   â”‚   â””â”€â”€ quality_check.py    # Quality validation
â”‚   â”œâ”€â”€ translate/              # (Future: translation scripts)
â”‚   â”œâ”€â”€ validate/               # (Future: validation scripts)
â”‚   â””â”€â”€ test/                   # (Future: bias testing)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ logs/                   # Execution logs
â”‚   â”œâ”€â”€ reports/                # Quality reports & statistics
â”‚   â”œâ”€â”€ metrics/                # (Future: bias metrics)
â”‚   â””â”€â”€ figures/                # (Future: visualizations)
â”‚
â”œâ”€â”€ validation/                 # (Future: annotation files)
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example                # API key template
â”œâ”€â”€ .gitignore                  # Git ignore rules
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”§ Installation

### Prerequisites

- **Python:** 3.10 or higher
- **Git:** For version control and cloning AMQA
- **Internet:** For downloading datasets

### Step 1: Clone or Navigate to Project

```powershell
cd "c:\Users\bashi\OneDrive\Desktop\Thesis\Bias Framework\Implementation Part 1"
```

### Step 2: Install Python Dependencies

```powershell
pip install -r requirements.txt
```

**If you're on a system-managed Python:**

```powershell
pip install -r requirements.txt --break-system-packages
```

### Step 3: Verify Installation

```powershell
python -c "import pandas; import datasets; print('âœ“ All dependencies installed')"
```

---

## ğŸš€ Usage Guide

### Stage 1.1: Download Datasets

**Script:** `scripts/download/download_medqa.py`

```powershell
python scripts/download/download_medqa.py
```

**What it does:**

- Downloads MedQA-USMLE dataset from HuggingFace (~12,700 questions)
- Clones AMQA repository from GitHub
- Saves data to `data/raw/`

**Expected output:**

```
============================================================
MedQA & AMQA Dataset Download
============================================================
âœ“ Downloaded 12,723 questions
âœ“ Saved to: data/raw/medqa_usmle_full.json
âœ“ Cloned AMQA to data/raw/amqa
âœ“ All Downloads Complete!
```

**Time:** 2-5 minutes (depending on internet speed)

---

### Stage 1.2: Explore Dataset

**Script:** `scripts/filter/explore_data.py`

```powershell
python scripts/filter/explore_data.py
```

**What it does:**

- Analyzes dataset structure and fields
- Calculates question length statistics
- Identifies clinical vignettes
- Analyzes demographic mentions (age, gender)
- Categorizes questions by medical topic
- Generates exploration report

**Expected output:**

```
============================================================
DATASET STRUCTURE ANALYSIS
============================================================
Total questions: 12,723
Fields available: question, options, answer, answer_idx, ...

Question Length Statistics:
  Average: 487 characters
  Clinical Vignettes: ~8,456 (66.5%)

âœ“ Exploration report saved to: outputs/reports/stage1_exploration_summary.txt
```

**Time:** 1-2 minutes

---

### Stage 1.3: Filter & Select Vignettes

**Script:** `scripts/filter/filter_vignettes.py`

```powershell
python scripts/filter/filter_vignettes.py
```

**What it does:**

- Applies filtering criteria:
  1. Must be clinical vignette (patient presentation)
  2. Must be demographically neutral (no gender-specific conditions)
  3. Must have exactly 4 answer options
  4. Must be reasonable length (150-2000 characters)
- Categorizes by medical domain
- Performs stratified sampling to select 500 vignettes
- Saves filtered and selected datasets

**Filtering criteria details:**

| Filter              | Description                                   | Example                              |
| ------------------- | --------------------------------------------- | ------------------------------------ |
| Clinical Vignette   | Contains patient, age, presentation, symptoms | "A 45-year-old man presents with..." |
| Demographic Neutral | No pregnancy, gender-specific organs          | âœ— "Pregnant woman..."                |
| 4 Options           | Exactly 4 multiple-choice answers             | A, B, C, D                           |
| Length              | 150-2000 characters                           | Not too short/long                   |

**Target distribution (Bangladesh disease burden):**

- Infectious diseases: 30% (150)
- Diabetes: 20% (100)
- Cardiovascular: 20% (100)
- Respiratory: 15% (75)
- Other conditions: 15% (75)

**Expected output:**

```
============================================================
FILTERING STATISTICS
============================================================
Total questions: 12,723
  Clinical vignettes:     8,456 (66.5%)
  Demographic neutral:    6,234 (49.0%)
  Has 4 options:          5,892 (46.3%)
  Reasonable length:      4,127 (32.4%)

âœ“ Final filtered:         2,156 (16.9%)

============================================================
STRATIFIED SAMPLING
============================================================
âœ“ Selected 500 vignettes

Final category breakdown:
  infectious:         150 (30.0%)
  diabetes:           100 (20.0%)
  cardiovascular:     100 (20.0%)
  respiratory:         75 (15.0%)
  gastrointestinal:    40 ( 8.0%)
  neurological:        20 ( 4.0%)
  renal:              15 ( 3.0%)
```

**Output files:**

- `data/filtered/medqa_filtered_all.json` - All filtered vignettes (~2,156)
- `data/filtered/medqa_selected_500.json` - Selected 500 vignettes
- `outputs/reports/stage1_filtering_stats.json` - Statistics

**Time:** 2-3 minutes

---

### Stage 1.4: Quality Check

**Script:** `scripts/filter/quality_check.py`

```powershell
python scripts/filter/quality_check.py
```

**What it does:**

- Validates completeness (all required fields present)
- Checks length distribution
- Validates clinical vignette structure
- Verifies category distribution
- Checks for diversity (age groups, no duplicates)
- Ensures ID uniqueness
- Generates quality report

**Expected output:**

```
============================================================
QUALITY CHECK REPORT
============================================================

âœ“ Completeness:           PASS âœ“
âœ“ Length Distribution:    PASS âœ“
âœ“ Readability:            PASS âœ“
âœ“ Category Distribution:  PASS âœ“
âœ“ Diversity:              PASS âœ“
âœ“ ID Uniqueness:          PASS âœ“

Overall: PASS âœ“

âœ“âœ“âœ“ ALL CHECKS PASSED âœ“âœ“âœ“
```

**Output:**

- `outputs/reports/stage1_quality_check.txt` - Detailed quality report

**Time:** <1 minute

---

## ğŸ“¦ What You Need to Provide

### Required:

1. **Python 3.10+** installed
2. **Internet connection** for downloading datasets
3. **~500 MB disk space** for datasets

### NOT Required at This Stage:

- âŒ API keys (OpenAI, Anthropic) - needed for Part 2 (translation)
- âŒ Medical experts - needed for Part 3 (validation)
- âŒ LLM access - needed for Part 4 (bias testing)

### To Continue to Part 2 (Translation):

You will need:

- OpenAI API key (GPT-4o) - ~$5 for 500 translations
- Anthropic API key (Claude 3.5) - ~$3 for validation
- Create `.env` file from `.env.example` and add keys

---

## ğŸ” Troubleshooting

### Issue: "datasets library not installed"

```powershell
pip install datasets --upgrade
```

### Issue: "Git not found" (for AMQA download)

- **Option 1:** Install Git from https://git-scm.com/
- **Option 2:** Manually download AMQA:
  1. Visit: https://github.com/xy-showing/amqa
  2. Download ZIP
  3. Extract to `data/raw/amqa/`

### Issue: "Only X vignettes after filtering"

- This is normal if dataset structure differs
- Check `outputs/reports/stage1_filtering_stats.json` for details
- You may need to adjust filtering criteria in `filter_vignettes.py`

### Issue: "HuggingFace download slow"

- Large dataset (~100 MB)
- Wait for download to complete
- Or download manually and place in `data/raw/`

### Issue: "Permission denied" on Windows

```powershell
# Run PowerShell as Administrator
pip install -r requirements.txt
```

---

## ğŸ“Š Expected Outputs After Part 1

After completing all scripts, you should have:

### Files:

```
data/filtered/
â”œâ”€â”€ medqa_filtered_all.json      # ~2,156 filtered vignettes
â””â”€â”€ medqa_selected_500.json      # 500 selected vignettes âœ“

outputs/reports/
â”œâ”€â”€ stage1_exploration_summary.txt
â”œâ”€â”€ stage1_filtering_stats.json
â””â”€â”€ stage1_quality_check.txt
```

### Key Metrics:

- âœ… **500 vignettes** selected and validated
- âœ… **Stratified by disease category** (infectious 30%, diabetes 20%, etc.)
- âœ… **All clinical vignettes** (patient presentations)
- âœ… **Demographically neutral** (adaptable for bias testing)
- âœ… **Quality validated** (completeness, length, structure)

---

## ğŸ¯ Next Steps

### Part 2: Translation (Coming Next)

Once Part 1 is complete, you'll proceed to:

1. **Dual-LLM Translation Pipeline**
   - GPT-4o translates English â†’ Bangla
   - Claude 3.5 validates translations
   - Cost: ~$8 total

2. **Expert Validation**
   - 3 Bangladeshi doctors review
   - Annotate quality, accuracy, context

3. **Bangladesh Context Adaptation**
   - Adapt demographics (urban/rural, wealth)
   - Create 6 variants per vignette

### To Prepare for Part 2:

1. Get OpenAI API key: https://platform.openai.com/api-keys
2. Get Anthropic API key: https://console.anthropic.com/
3. Create `.env` file:
   ```bash
   cp .env.example .env
   # Edit .env and add your keys
   ```

---

## ğŸ“ˆ Project Timeline

| Stage                 | Status     | Duration   |
| --------------------- | ---------- | ---------- |
| **Part 1: Selection** | âœ… READY   | 1-2 days   |
| Part 2: Translation   | ğŸ”œ Next    | 3-4 days   |
| Part 3: Validation    | â³ Pending | 2-3 weeks  |
| Part 4: Adaptation    | â³ Pending | 1 week     |
| Part 5: Testing       | â³ Pending | 2-3 weeks  |
| Part 6: Publication   | â³ Pending | 8-10 weeks |

---

## ğŸ¤ Support

If you encounter issues:

1. **Check troubleshooting section above**
2. **Review error messages carefully**
3. **Check file paths** (especially on Windows with spaces)
4. **Verify Python version:** `python --version` (should be 3.10+)
5. **Check internet connection** for downloads

---

## ğŸ“ Notes

- All scripts use **relative paths** from project root
- **Random seed** is set (42) for reproducibility
- **Data files** in `data/raw/` are gitignored (too large)
- **Output reports** help track progress and quality
- **Part 1 is standalone** - no API keys needed yet

---

## âœ… Checklist

Before moving to Part 2:

- [ ] All dependencies installed (`pip install -r requirements.txt`)
- [ ] MedQA dataset downloaded (~12,723 questions)
- [ ] AMQA repository cloned
- [ ] Dataset explored (exploration report generated)
- [ ] Vignettes filtered (~2,156 candidates)
- [ ] 500 vignettes selected (stratified sampling)
- [ ] Quality check passed (all checks âœ“)
- [ ] Output files exist in `data/filtered/`

**If all checks pass, you're ready for Part 2: Translation!**

---

**Last Updated:** February 2026  
**Version:** 1.0  
**Stage:** Part 1 - Dataset Preparation
